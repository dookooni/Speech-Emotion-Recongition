import torch
import torch.nn as nn
import torch.optim as optim
from transformers import Wav2Vec2Processor, Wav2Vec2Config
from accelerate import Accelerator
from torch.utils.data import DataLoader
import warnings
warnings.filterwarnings('ignore')

from config import config
from datasets import EmotionDataset, collate_fn, preprocess_audio
from utils import setup_logging, CheckpointManager, validate_audio_files
from data_utils import (split_speaker_and_content, build_corpus_index, extract_speaker_id, extract_number_from_filename,
                       build_large_corpus_index, balance_large_dataset, split_large_dataset, balance_by_undersampling_majority)  # split_large_dataset ì¶”ê°€
from Wav2Vec2_seq_clf import custom_Wav2Vec2ForEmotionClassification, HybridEmotionModel
from model_utils import enable_last_k_blocks, enable_hybrid_last_k_blocks

from collections import Counter
from tqdm import tqdm
from sklearn.metrics import classification_report, accuracy_score, f1_score

import wandb
import random

def create_model_and_processor(freeze_base_model: bool = True, num_speakers: int = 500):
    """ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œ ìƒì„±"""
    print(f"ğŸ¤– ëª¨ë¸ ë¡œë”©: {config.model.model_name}")
    
    # ì„¤ì • ìˆ˜ì •
    model_config = Wav2Vec2Config.from_pretrained(
        config.model.model_name,
        num_labels=config.model.num_labels,
        label2id=config.model.label2id,
        id2label=config.model.id2label,
        finetuning_task="emotion_classification"
    )
    model_config.num_speakers = num_speakers  # í™”ì ìˆ˜ ì„¤ì •
    
    if config.char_vocab:
        model_config.char_vocab_size = len(config.char_vocab)
    
    model = custom_Wav2Vec2ForEmotionClassification.from_pretrained(
        config.model.model_name,
        config=model_config,
        ignore_mismatched_sizes=True
    )
    
    processor = Wav2Vec2Processor.from_pretrained(config.model.model_name)
    
    if freeze_base_model:
        for param in model.wav2vec2.parameters():
            param.requires_grad = False
        print("âœ… Base model íŒŒë¼ë¯¸í„° ê³ ì •")
    
    return model, processor

def create_hybrid_model_and_processor(config, freeze_w2v2_base: bool = True):
    """í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ê³¼ Wav2Vec2 í”„ë¡œì„¸ì„œë¥¼ ìƒì„±"""
    
    w2v2_model_name = config.model.w2v2_model_name
    prosody_model_name = config.model.prosody_model_name
    num_labels = config.model.num_labels
    
    print(f"ğŸ¤– ë‚´ìš© ì „ë¬¸ê°€ ë¡œë”©: {w2v2_model_name}")
    print(f"ğŸ¶ í”„ë¡œì†Œë”” ì „ë¬¸ê°€ ë¡œë”©: {prosody_model_name}")
    
    # 1. Wav2Vec2 í”„ë¡œì„¸ì„œ ë¡œë”© (ë°ì´í„° ì „ì²˜ë¦¬ì— ì‚¬ìš©)
    processor = Wav2Vec2Processor.from_pretrained(w2v2_model_name)
    
    # 2. í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    model = HybridEmotionModel(
        w2v2_model_name=w2v2_model_name,
        prosody_model_name=prosody_model_name,
        num_labels=num_labels,
        freeze_w2v2=freeze_w2v2_base,
        freeze_prosody=True  # í”„ë¡œì†Œë”” ëª¨ë¸ì€ í•­ìƒ ë™ê²°í•˜ëŠ” ê²ƒì„ ì¶”ì²œ
    )
    
    if freeze_w2v2_base:
        print("âœ… ë‚´ìš© ì „ë¬¸ê°€(Wav2Vec2) Base model íŒŒë¼ë¯¸í„° ê³ ì •")
    print("âœ… í”„ë¡œì†Œë”” ì „ë¬¸ê°€(Speechbrain) íŒŒë¼ë¯¸í„° ê³ ì •")
    
    return model, processor

def train_model(model, train_loader, val_loader, device, num_epochs=3, learning_rate=3e-5):
    """ì§ì ‘ í›ˆë ¨ ë£¨í”„"""
    
    # ì˜µí‹°ë§ˆì´ì € ì„¤ì • (ì°¨ë“± í•™ìŠµë¥  ì ìš©)
    print("ğŸš€ ì˜µí‹°ë§ˆì´ì € ì„¤ì • (ì°¨ë“± í•™ìŠµë¥  ì ìš©)")
    backbone_params = []
    head_params = []

    for n, p in model.named_parameters():
        if not p.requires_grad:
            continue
        if n.startswith("wav2vec2."):
            backbone_params.append(p)
        # if n.startswith("w2v2."):
        #     backbone_params.append(p)
        else:
            # pooler, stats_projector, projector(ë¶€ëª¨), classifier, adversaries ë“±
            head_params.append(p)

    optimizer = optim.AdamW(
        [
            {"params": backbone_params, "lr": 5e-6, "weight_decay": 0.01},
            {"params": head_params,     "lr": 1e-4, "weight_decay": 0.01},
        ]
    )
    
    # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
    total_steps = len(train_loader) * num_epochs
    accumulation_steps = 8
    scheduler_steps = total_steps // accumulation_steps
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=scheduler_steps)
    
    best_f1 = 0
    best_model_state = None
    
    print(f"\nğŸš€ í›ˆë ¨ ì‹œì‘!")
    print(f"   ì´ ì—í¬í¬: {num_epochs}")
    print(f"   ë°°ì¹˜ ìˆ˜: {len(train_loader)}")
    print(f"   ì´ ìŠ¤í…: {total_steps}")
    
    max_adv = 0.1
    warmup_epochs = 1.0
    # 4í´ë˜ìŠ¤ì— ë§ê²Œ ê°€ì¤‘ì¹˜ ì¡°ì • (Anxious, Dry, Kind, Other ìˆœì„œ)
    class_weights = torch.tensor([2.0, 1.5, 0.7, 0.5], dtype=torch.float32).to(device)  # OtherëŠ” ê°€ì¤‘ì¹˜ë¥¼ ë‚®ê²Œ

    for epoch in range(num_epochs):
        print(f"\n=== Epoch {epoch + 1}/{num_epochs} ===")
        
        # í›ˆë ¨
        model.train()
        train_loss = 0
        train_predictions = []
        train_true_labels = []
        optimizer.zero_grad()

        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1} Training")
        for step, batch in enumerate(progress_bar):
            current_step = epoch * len(train_loader) + step
            total_warmup_steps = warmup_epochs * len(train_loader)
            progress = min(1.0, current_step / total_warmup_steps)
            current_adv_lambda = max_adv * progress
            # current_adv_lambda = 0.0
            
            outputs = model(
                input_values=batch['input_values'].to(device),
                attention_mask=batch['attention_mask'].to(device),
                labels=batch['labels'].to(device),
                content_labels=batch['content_labels'].to(device),
                content_labels_lengths=batch['content_labels_lengths'].to(device),
                adv_lambda=current_adv_lambda,
                speaker_ids=batch['speaker_ids'].to(device),
                class_weights = class_weights
            )

            # outputs = model(
            #     input_values=batch['input_values'].to(device),
            #     labels = batch['labels'].to(device),
            #     class_weights = class_weights,
            # )
            
            loss = outputs['loss']
            if loss is None or torch.isnan(loss) or torch.isinf(loss):
                continue

            loss /= accumulation_steps
            loss.backward()

            if (step + 1) % accumulation_steps == 0:
              torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
              optimizer.step()
              scheduler.step()
              optimizer.zero_grad()
            
            train_loss += loss.item()
            
            # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
            preds = torch.argmax(outputs['emotion_logits'], dim=-1)
            # preds = torch.argmax(outputs['logits'], dim=-1)
            train_predictions.extend(preds.cpu().numpy())
            train_true_labels.extend(batch['labels'].cpu().numpy())
            
            progress_bar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'lr': f'{scheduler.get_last_lr()[0]:.6f}'
            })
        
        train_loss /= len(train_loader)
        
        # í›ˆë ¨ ì •í™•ë„ ë° F1 ê³„ì‚°
        train_accuracy = accuracy_score(train_true_labels, train_predictions)
        train_f1 = f1_score(train_true_labels, train_predictions, average='weighted')

        # Weight & Biases ë¡œê¹…
        wandb.log({
            "epoch": epoch,
            "train/loss": train_loss,
            "train/accuracy": train_accuracy,
            "train/f1": train_f1
        })

        # ê²€ì¦
        print("ğŸ“Š ê²€ì¦ ì¤‘...")
        val_results = evaluate_model(model, val_loader, device, epoch=epoch, is_training=True)
        
        print(f"ğŸ” ê²€ì¦ ê²°ê³¼ - Loss: {val_results['loss']:.4f}, Acc: {val_results['accuracy']:.4f}, F1: {val_results['f1']:.4f}")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
        if val_results['f1'] > best_f1:
            best_f1 = val_results['f1']
            best_model_state = model.state_dict().copy()
            print(f"âœ¨ ìƒˆë¡œìš´ ìµœê³  F1 ì ìˆ˜: {best_f1:.4f}")
    
    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
        print(f"\nğŸ’« ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (F1: {best_f1:.4f})")
    
    return model

def evaluate_model(model, dataloader, device, epoch=None, is_training=False):
    """ëª¨ë¸ í‰ê°€"""
    model.eval()
    total_loss = 0
    predictions = []
    true_labels = []
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="í‰ê°€ ì¤‘"):
            
            outputs = model(
                input_values=batch['input_values'].to(device),
                attention_mask=batch['attention_mask'].to(device),
                labels=batch['labels'].to(device),
                content_labels=batch['content_labels'].to(device),
                content_labels_lengths=batch['content_labels_lengths'].to(device),
                adv_lambda=0.0, # í‰ê°€ ì‹œì—ëŠ” ì ëŒ€ì  ì†ì‹¤ ë°˜ì˜ ì•ˆí•¨,
                speaker_ids = None,
            )

            # outputs = model(
            #     input_values=batch['input_values'].to(device),
            #     labels = batch['labels'].to(device),
            # )       
            
            loss = outputs['loss']
            
            # í›ˆë ¨ ì¤‘ì´ ì•„ë‹ ë•Œë„ lossê°€ ê³„ì‚°ë˜ë„ë¡ adv_lambda=0.0ìœ¼ë¡œ í˜¸ì¶œ
            # ë§Œì•½ lossê°€ Noneì´ë©´ (test set ë“±ì—ì„œ content_labelì´ ì—†ì„ ê²½ìš°), ê°ì • ì†ì‹¤ë§Œ ê³„ì‚°
            if loss is None:
                loss_fct = nn.CrossEntropyLoss()
                loss = loss_fct(outputs['emotion_logits'].view(-1, model.config.num_labels), batch['labels'].to(device).view(-1))

            total_loss += loss.item()
            
            # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
            preds = torch.argmax(outputs['emotion_logits'], dim=-1)
            # preds = torch.argmax(outputs['logits'], dim=-1)
            predictions.extend(preds.cpu().numpy())
            true_labels.extend(batch['labels'].cpu().numpy())
    
    avg_loss = total_loss / len(dataloader)
    accuracy = accuracy_score(true_labels, predictions)
    f1 = f1_score(true_labels, predictions, average='weighted')

    if is_training:
        wandb.log({
            "epoch" : epoch,
            "val/loss": avg_loss,
            "val/accuracy": accuracy,
            "val/f1": f1,
        })
    
    return {
        'loss': avg_loss,
        'accuracy': accuracy,
        'f1': f1,
        'predictions': predictions,
        'true_labels': true_labels
    }

def main():
    """ë©”ì¸ í›ˆë ¨ í•¨ìˆ˜"""
    logger = setup_logging("INFO", "training.log")
    logger.info("ğŸš€ SER ëª¨ë¸ í›ˆë ¨ ì‹œì‘")
    
    logger.info(f"ëª¨ë¸: {config.model.model_name}")
    logger.info(f"ê°ì • ë¼ë²¨: {config.model.emotion_labels}")
    logger.info(f"ë°°ì¹˜ í¬ê¸°: {config.training.batch_size}")
    logger.info(f"í•™ìŠµë¥ : {config.training.learning_rate}")
    
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    logger.info(f"ì‚¬ìš© ì¥ì¹˜: {device}")
    # accelerator = Accelerator()

    # Weight & Biases ì´ˆê¸°í™”
    wandb.init(
    project="Speech_Emotion_Recognition",
    name="Adversary 4 Class Classification Large Dataset Balancing",

    config={
        "learning_rate": config.training.learning_rate,
        "epochs": config.training.num_epochs,
        "batch_size": config.training.batch_size,
        "architecture": "HybridEmotionModel",
    }
    )

    # ë°ì´í„° ë¡œë“œ (Small vs Large ë°ì´í„°ì…‹ ì„ íƒ)
    if config.data.use_large_dataset:
        print("ğŸ“Š Large ë°ì´í„°ì…‹ ì‚¬ìš© ì¤‘...")
        print(f"ğŸ“ Large ë°ì´í„° ê²½ë¡œ: {config.paths.large_data_dir}")
        
        # Large ë°ì´í„°ì…‹ ì¸ë±ìŠ¤ ìƒì„±
        large_index = build_large_corpus_index(str(config.paths.large_data_dir))
        if not large_index:
            print("âŒ Large ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨. Small ë°ì´í„°ì…‹ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            config.data.use_large_dataset = False
        else:
            # í´ë˜ìŠ¤ ê· í˜• ì¡°ì •
            print(f"âš–ï¸ í´ë˜ìŠ¤ ê· í˜• ì¡°ì • ì¤‘ (ë¹„ìœ¨: {config.data.large_balance_ratio})")
            index = balance_by_undersampling_majority(large_index)
            print(f"âœ… ê· í˜• ì¡°ì • ì™„ë£Œ - ìµœì¢… ìƒ˜í”Œ ìˆ˜: {len(index)}ê°œ")
    
    if not config.data.use_large_dataset:
        print("ğŸ“Š Small ë°ì´í„°ì…‹ ì‚¬ìš© ì¤‘ (í´ë˜ìŠ¤ ê· í˜• ë§ì¶”ê¸°)...")
        
        # ë¨¼ì € ì „ì²´ ë°ì´í„° í™•ì¸
        full_index = build_corpus_index(config.paths.data_dir)
        emotion_counts = Counter([item["emotion"] for item in full_index])
        print(f"ì „ì²´ í´ë˜ìŠ¤ ë¶„í¬: {dict(emotion_counts)}")
        
        # ê°€ì¥ ì ì€ í´ë˜ìŠ¤ì˜ ìƒ˜í”Œ ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì„¤ì • (ë˜ëŠ” ì›í•˜ëŠ” ìµœëŒ€ê°’ ì„¤ì •)
        min_samples = min(emotion_counts.values())
        max_samples_per_class = min(min_samples * 2, 1000)  # ìµœì†Œ í´ë˜ìŠ¤ì˜ 2ë°° ë˜ëŠ” 1000ê°œ ì¤‘ ì‘ì€ ê°’
        print(f"í´ë˜ìŠ¤ë‹¹ ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ë¥¼ {max_samples_per_class}ê°œë¡œ ì œí•œí•©ë‹ˆë‹¤.")
        
        # ê· í˜• ì¡íŒ ì¸ë±ìŠ¤ ìƒì„±
        index = build_corpus_index(config.paths.data_dir, max_samples_per_class=max_samples_per_class)
        
    valid_paths, invalid_paths = validate_audio_files([item["path"] for item in index])
    
    if invalid_paths:
        logger.warning(f"ìœ íš¨í•˜ì§€ ì•Šì€ íŒŒì¼ {len(invalid_paths)}ê°œ ì œì™¸")
    
    # ë°ì´í„° ë¶„í•  (Large vs Small ë°ì´í„°ì…‹ì— ë”°ë¼ ë‹¤ë¥¸ í•¨ìˆ˜ ì‚¬ìš©)
    if config.data.use_large_dataset:
        (train_paths, train_labels), (val_paths, val_labels), (test_paths, test_labels) = \
            split_large_dataset(
                index,
                val_speaker_ratio=0.2,
                test_speaker_ratio=0.2,
                val_content_ratio=0.2,
                test_content_ratio=0.2,
            )
        # Large ë°ì´í„°ì…‹ìš© í™”ì ì¶”ì¶œ (í´ë”ëª… ê¸°ì¤€)
        train_speakers = sorted(set([item["speaker"] for item in index if item["path"] in train_paths]))
    else:
        (train_paths, train_labels), (val_paths, val_labels), (test_paths, test_labels) = \
            split_speaker_and_content(
                index,
                val_content_ratio=0.2,
                test_content_ratio=0.2,
                val_speaker_ratio=0.2,
                test_speaker_ratio=0.2,
            )
        # Small ë°ì´í„°ì…‹ìš© í™”ì ì¶”ì¶œ
        train_speakers = sorted({extract_speaker_id(p, config.paths.data_dir) for p in train_paths})
    num_speakers = len(train_speakers)
    print(f"ğŸ” í™”ì ìˆ˜: {num_speakers}")

    model, processor = create_model_and_processor(num_speakers=num_speakers)
    # model, processor = create_hybrid_model_and_processor(config)
    enable_last_k_blocks(model, last_k=4)
    # enable_hybrid_last_k_blocks(model, last_k=4)
    model.to(device)

    print(f"\nğŸ“Š ë¶„í•  ê²°ê³¼:")
    print(f"  Train: {len(train_paths)}ê°œ")
    print(f"  Validation: {len(val_paths)}ê°œ")
    print(f"  Test: {len(test_paths)}ê°œ")

    train_emotion_dist = Counter(train_labels)
    val_emotion_dist = Counter(val_labels)
    test_emotion_dist = Counter(test_labels)
    
    print(f"\nğŸ“ˆ ê°ì •ë³„ ë¶„í¬:")
    print(f"  Train: {dict(train_emotion_dist)}")
    print(f"  Validation: {dict(val_emotion_dist)}")
    print(f"  Test: {dict(test_emotion_dist)}")
    
    # ë°ì´í„°ì…‹ ìƒì„±
    train_dataset = EmotionDataset(train_paths, train_labels, processor, is_training=True)
    val_dataset = EmotionDataset(val_paths, val_labels, processor, is_training=False)
    test_dataset = EmotionDataset(test_paths, test_labels, processor, is_training=False)
    
    # ë°ì´í„°ë¡œë” ìƒì„±
    train_loader = DataLoader(
        train_dataset, 
        batch_size=config.training.batch_size,
        shuffle=True,
        collate_fn=collate_fn,
        drop_last=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.training.batch_size,
        shuffle=False,
        collate_fn=collate_fn,
        drop_last=True,
    )

    test_loader = DataLoader(
        test_dataset,
        batch_size=config.training.batch_size,
        shuffle=False,
        collate_fn=collate_fn,
        drop_last=True,
    )

    # ì²´í¬í¬ì¸íŠ¸ ë§¤ë‹ˆì €
    checkpoint_manager = CheckpointManager()
    
    # í›ˆë ¨ ì‹¤í–‰
    try:
        # í›ˆë ¨ ì‹¤í–‰ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš© 1ì—í¬í¬)
        model = train_model(model, 
                            train_loader, 
                            val_loader, 
                            device, 
                            num_epochs=config.training.num_epochs, 
                            learning_rate=config.training.learning_rate)
        
        # ìµœì¢… í…ŒìŠ¤íŠ¸ í‰ê°€
        print(f"\nğŸ§ª ìµœì¢… í…ŒìŠ¤íŠ¸ í‰ê°€...")
        test_results = evaluate_model(model, test_loader, device, is_training=False)
        
        print(f"\nğŸ¯ ìµœì¢… í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        print(f"   - ì •í™•ë„: {test_results['accuracy']:.4f}")
        print(f"   - F1 ìŠ¤ì½”ì–´: {test_results['f1']:.4f}")
        print(f"   - ì†ì‹¤: {test_results['loss']:.4f}")
        
        # ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸
        print(f"\nğŸ“‹ ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸:")
        report = classification_report(
            test_results['true_labels'], 
            test_results['predictions'],
            target_names=config.model.emotion_labels,
            digits=4
        )
        print(report)
        
        # í•™ìŠµëœ ëª¨ë¸ ì €ì¥
        print(f"\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
        output_dir = config.paths.output_dir
        import os
        os.makedirs(output_dir, exist_ok=True)
        
        # ëª¨ë¸ ìƒíƒœ ì €ì¥ (transformers ë°©ì‹)
        model_save_path = os.path.join(output_dir, "Adversary_class4_v2_LargeDataset_DataBalancing")
        os.makedirs(model_save_path, exist_ok=True)
        
        # ëª¨ë¸ ì €ì¥
        try:
            model.save_pretrained(model_save_path)
            # torch.save(model.state_dict(), os.path.join(model_save_path, "model.pt"))
            processor.save_pretrained(model_save_path)
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            torch.save(model.state_dict(), os.path.join(model_save_path, "model.pt"))
        
        print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_save_path}")
        print(f"\nğŸ‰ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print(f"   ëª¨ë“  íŒŒì´í”„ë¼ì¸ì´ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.")
        print(f"   ì €ì¥ëœ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸: python test_my_voice.py your_audio.wav --model_path {model_save_path}")
        
    except KeyboardInterrupt:
        print(f"\nâ¹ï¸  í›ˆë ¨ì´ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    
    logger.info("âœ… í›ˆë ¨ ì™„ë£Œ")





if __name__ == "__main__":
    main()