# SER (Speech Emotion Recognition) ëª¨ë“ˆ

í•œêµ­ì–´ ìŒì„± ê°ì • ë¶„ì„ì„ ìœ„í•œ Wav2Vec2 ê¸°ë°˜ Fine-tuning ëª¨ë“ˆì…ë‹ˆë‹¤.

## ğŸ“‹ ê°œìš”

ì´ ëª¨ë“ˆì€ Hugging Faceì˜ [`kresnik/wav2vec2-large-xlsr-korean`](https://huggingface.co/kresnik/wav2vec2-large-xlsr-korean) ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ 4ê°€ì§€ ê°ì •ì„ ë¶„ë¥˜í•˜ëŠ” ìŒì„± ê°ì • ë¶„ì„ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

### ğŸ¯ ê¸°ë°˜ ëª¨ë¸ ì •ë³´
- **ëª¨ë¸**: [kresnik/wav2vec2-large-xlsr-korean](https://huggingface.co/kresnik/wav2vec2-large-xlsr-korean)
- **íŒŒë¼ë¯¸í„° ìˆ˜**: 317M
- **ì›ë³¸ ì„±ëŠ¥**: WER 4.74%, CER 1.78% (Zeroth-Korean ASR)
- **ìƒ˜í”Œë§ ë ˆì´íŠ¸**: 16kHz
- **ë¼ì´ì„ ìŠ¤**: Apache 2.0

### ì§€ì›í•˜ëŠ” ê°ì • í´ë˜ìŠ¤
- Anxious (ë¶ˆì•ˆ)
- Kind (ì¹œì ˆ)
- Dry (ê±´ì¡°í•¨)
- Others (ê¸°íƒ€)

## ğŸ—ï¸ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
SER/
â”œâ”€â”€ Sagemaker/
    â”œâ”€â”€ code /                    # Sagemaker ì¶”ë¡ ì„ ìœ„í•œ model.tar.gz íŒŒì¼ êµ¬ì„±
    â”œâ”€â”€ deploy.py                 # Sagemaker ë°°í¬
    â”œâ”€â”€ Dockerfile                # Sagemaker ë°°í¬ìš© ë„ì»¤ ì´ë¯¸ì§€
â”œâ”€â”€ config.py                     # ì„¤ì • ê´€ë¦¬
â”œâ”€â”€ model.py                      # Wav2Vec2 ëª¨ë¸ êµ¬í˜„
â”œâ”€â”€ preprocessing.py              # ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬
â”œâ”€â”€ datasets.py                   # ë°ì´í„° ë¡œë”
â”œâ”€â”€ trainer.py                    # 1 GPU í›ˆë ¨
â”œâ”€â”€ trainer_accelerate.py         # Multi GPU í›ˆë ¨
â”œâ”€â”€ trainer_accelerate_text.py    # Text ë²„ì „ Multi GPU í›ˆë ¨
â”œâ”€â”€ test_dataset.py               # ëª¨ë¸ í‰ê°€
â”œâ”€â”€ utils.py                      # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
â”œâ”€â”€ data_utils.py                 # ë°ì´í„° ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
â”œâ”€â”€ model_utils.py                # ëª¨ë¸ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
â”œâ”€â”€ requirements.txt              # ì˜ì¡´ì„±
â””â”€â”€ README.md                     # ë¬¸ì„œ
```

## âš¡ ë¹ ë¥¸ ì‹œì‘

### 1. ì˜ì¡´ì„± ì„¤ì¹˜

```bash
cd Backend/SER
pip install -r requirements.txt
```

### 2. ê¸°ë³¸ ì‚¬ìš©ë²•

#### ëª¨ë¸ í›ˆë ¨

```bash
CUDA_VISIBLE_DEVICES=0,1,2 accelerate launch --num_processes=3 trainer_accelerate_text.py conda activate ser2
```

#### ì¶”ë¡ 

#### ëª¨ë¸ í‰ê°€

```bash
python -m SER.evaluate \
    --model_path ./results \
    --data_dir ./test_data \
    --output_dir ./evaluation_results
```


## ğŸ“Š ë°ì´í„° ì¤€ë¹„

### ë””ë ‰í† ë¦¬ êµ¬ì¡° ë°©ì‹

```
data/
â”œâ”€â”€ F0001/
â”‚   â”œâ”€â”€ wav_48000/
â”‚   â”‚   â”œâ”€â”€ file1.wav
â”‚   â”‚   â””â”€â”€ file2.wav
â”‚   â”œâ”€â”€ script.txt
â”‚   â””â”€â”€ ...
```

### CSV íŒŒì¼ ë°©ì‹

```csv
file_path,emotion
/path/to/audio1.wav,Neutral
/path/to/audio2.wav,Angry
/path/to/audio3.wav,Joy
...
```

## âš™ï¸ ì„¤ì •

### ëª¨ë¸ ì„¤ì • (`config.py`)

```python
from SER.config import model_config

# ê¸°ë³¸ ì„¤ì • í™•ì¸
print(f"ëª¨ë¸: {model_config.model_name}")
print(f"ê°ì • í´ë˜ìŠ¤: {model_config.emotion_labels}")

# ì„¤ì • ìˆ˜ì •
model_config.max_duration = 15.0  # ìµœëŒ€ ì˜¤ë””ì˜¤ ê¸¸ì´ ë³€ê²½
```

### í›ˆë ¨ ì„¤ì •

```python
from SER.config import training_config

# í›ˆë ¨ íŒŒë¼ë¯¸í„° ì¡°ì •
training_config.batch_size = 16
training_config.learning_rate = 5e-5
training_config.num_epochs = 20
```

## ğŸ“ˆ ëª¨ë¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

### í›ˆë ¨ ì¤‘ ëª¨ë‹ˆí„°ë§

```python
# í›ˆë ¨ íˆìŠ¤í† ë¦¬ ì‹œê°í™”
from SER.utils import plot_training_history

history = {
    'train_loss': [0.8, 0.6, 0.4, 0.3],
    'val_loss': [0.9, 0.7, 0.5, 0.4],
    'val_accuracy': [0.6, 0.7, 0.8, 0.85],
    'val_f1': [0.55, 0.65, 0.75, 0.82]
}

plot_training_history(history, save_path='training_history.png')
```

### í‰ê°€ ë° ë¶„ì„

```python
from SER.evaluate import ModelEvaluator

# í‰ê°€ê¸° ìƒì„±
evaluator = ModelEvaluator("./results")

# ë°ì´í„°ì…‹ í‰ê°€
results = evaluator.evaluate_dataset(audio_paths, true_labels)

# í˜¼ë™ í–‰ë ¬ ì‹œê°í™”
evaluator.plot_confusion_matrix()

# í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ì‹œê°í™”
evaluator.plot_class_metrics()

# ë³´ê³ ì„œ ìƒì„±
report = evaluator.generate_evaluation_report()
print(report)
```

## ğŸ”§ ê³ ê¸‰ ì‚¬ìš©ë²•

### ì»¤ìŠ¤í…€ ë°ì´í„° ë¡œë”

```python
from SER.data_loader import SpeechEmotionDataset
from transformers import Wav2Vec2Processor

processor = Wav2Vec2Processor.from_pretrained("kresnik/wav2vec2-large-xlsr-korean")

dataset = SpeechEmotionDataset(
    data_paths=audio_paths,
    labels=labels,
    processor=processor,
    is_training=True
)
```

### ì»¤ìŠ¤í…€ ì „ì²˜ë¦¬

```python
from SER.preprocessing import AudioPreprocessor

preprocessor = AudioPreprocessor(
    target_sr=16000,
    max_duration=15.0,
    normalize=True
)

# ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬
processed_audio = preprocessor.preprocess(
    "audio_file.wav",
    apply_augmentation=True
)
```

### ë°°ì¹˜ ì¶”ë¡ 

```python
# ì—¬ëŸ¬ íŒŒì¼ ë™ì‹œ ì²˜ë¦¬
audio_files = ["file1.wav", "file2.wav", "file3.wav"]
results = inference.predict_batch(audio_files)

# ë””ë ‰í† ë¦¬ ì „ì²´ ì²˜ë¦¬
results = inference.predict_directory("./audio_folder")

# ê°ì • ë¶„í¬ ë¶„ì„
distribution = inference.analyze_emotions_distribution(results)
print(f"ê°€ì¥ ë§ì€ ê°ì •: {max(distribution['emotion_counts'], key=distribution['emotion_counts'].get)}")
```

## ğŸ¯ ìµœì í™” íŒ

### 1. kresnik ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° (ê¶Œì¥)

```python
# kresnik/wav2vec2-large-xlsr-korean (317M íŒŒë¼ë¯¸í„°)ì— ìµœì í™”ëœ ì„¤ì •
training_config.learning_rate = 3e-5  # í° ëª¨ë¸ì— ì í•©í•œ ë‚®ì€ í•™ìŠµë¥ 
training_config.batch_size = 4        # GPU ë©”ëª¨ë¦¬ ê³ ë ¤
training_config.gradient_accumulation_steps = 4  # íš¨ê³¼ì  ë°°ì¹˜ í¬ê¸° = 16
training_config.warmup_steps = 1000   # ë” ê¸´ ì›œì—…
training_config.num_epochs = 5        # í° ëª¨ë¸ì€ ì ì€ ì—í¬í¬ë¡œë„ íš¨ê³¼ì 

# GPU ë©”ëª¨ë¦¬ì— ë”°ë¥¸ ì¡°ì •
# RTX 3090/4090 (24GB): batch_size = 8, gradient_accumulation_steps = 2
# RTX 3080/4080 (12GB): batch_size = 4, gradient_accumulation_steps = 4  
# RTX 3070/4070 (8GB):  batch_size = 2, gradient_accumulation_steps = 8
```

### 2. ë°ì´í„° ì¦ê°•

```python
from SER.config import data_config

# ë°ì´í„° ì¦ê°• í™œì„±í™”
data_config.data_augmentation = True
data_config.apply_noise_reduction = True
```

### 3. ì¡°ê¸° ì¢…ë£Œ

```python
# ì¡°ê¸° ì¢…ë£Œ ì„¤ì •
training_config.early_stopping_patience = 5
training_config.early_stopping_threshold = 0.001
```

## ğŸ› ë¬¸ì œ í•´ê²°

### ì¼ë°˜ì ì¸ ë¬¸ì œë“¤

1. **CUDA ë©”ëª¨ë¦¬ ë¶€ì¡±**
   ```python
   training_config.batch_size = 4  # ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
   training_config.fp16 = True     # Mixed precision ì‚¬ìš©
   ```

2. **ì˜¤ë””ì˜¤ ë¡œë“œ ì‹¤íŒ¨**
   ```python
   # ì§€ì›ë˜ëŠ” í˜•ì‹: .wav, .mp3, .m4a, .flac
   # librosaì™€ soundfile ì„¤ì¹˜ í™•ì¸
   pip install librosa soundfile
   ```

3. **ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨**
   ```python
   # ì¸í„°ë„· ì—°ê²° í™•ì¸ (Hugging Face ëª¨ë¸ ë‹¤ìš´ë¡œë“œ)
   # ë˜ëŠ” ë¡œì»¬ì— ëª¨ë¸ ìºì‹œ í™•ì¸
   ```

### ë¡œê¹… í™œì„±í™”

```python
from SER.utils import setup_logging

# ë””ë²„ê·¸ ë¡œê¹… í™œì„±í™”
logger = setup_logging(log_level="DEBUG", log_file="ser_debug.log")
```

## ğŸ“š ì°¸ê³ ìë£Œ

- [Wav2Vec2 ë…¼ë¬¸](https://arxiv.org/abs/2006.11477)
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/)
- [librosa ë¬¸ì„œ](https://librosa.org/doc/latest/)

## ğŸ¤ ê¸°ì—¬í•˜ê¸°

ë²„ê·¸ ë¦¬í¬íŠ¸ë‚˜ ê¸°ëŠ¥ ì œì•ˆì€ ì´ìŠˆë¡œ ë“±ë¡í•´ì£¼ì„¸ìš”.

## ğŸ“„ ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„ ìŠ¤ í•˜ì— ë°°í¬ë©ë‹ˆë‹¤.

---

**ì°¸ê³ **: ì‹¤ì œ ë°ì´í„°ì…‹ì´ ì¤€ë¹„ë˜ë©´ `data_loader.py`ì˜ `prepare_sample_data()` í•¨ìˆ˜ë¥¼ ìˆ˜ì •í•˜ì—¬ ì‹¤ì œ ë°ì´í„° ê²½ë¡œë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.
